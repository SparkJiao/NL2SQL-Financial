## 项目结构

该项目使用Hydra进行参数配置和实验管理。
- conf: 维护所有实验的配置文件
- data: 数据处理模块，从原始数据生成tensor
  - collator：一些DataCollator类，用于合并batch
- general_util: 常用的工具模块
  - average_meter: 计数器，可以用于计算accuracy等指标
  - fsdp_utils: 用于初始化fairscale的fully sharded data parallel (fsdp)的函数
  - logger: 日志管理
  - metrics: 定义了基于sqlite进行计算的em指标
  - mixin: 有关metric维护的接口类
  - sqliteStructureTrans: 由官方提供的脚本转换而来，作用是对sql语句进行解析并生成可用于评估的结构体
  - tensorboard_helper: tensorboard管理
  - training_utils: 训练工具函数
- models: 模型
- scripts: 用于分析的脚本
  - bm25_table_recall: 评估bm25的召回率
  - error_analysis: 对照标注过滤错误的预测
- trainer_slurm_fsdp_v1.py: slurm上训练启动脚本，多卡分布式训练时使用FSDP，也支持非分布式model parallel（主要用于mt5-xl） 

其余细节部分可参考代码中的README.md文件。

## 数据

### 数据文件说明

数据文件保存在`ccks2022/ccks_nl2sql`目录下

- train.json 全量训练集，含标注。
- dev.json 验证集，用于日常提交，无标注。
- dev_gold.json 测试阶段公布的验证集，数据与dev.json一致，包含标注。
- sub_train.json 日常提交阶段因无验证集，从train.json选出了一部分数据当做验证集后剩下的训练集部分。
- sub_dev.json 同上，从train.json中选的部分数据，当做验证集。
- sub_dev_0626.json 纠正了sub_dev.json中部分标注错误（大概7条）
- train_4466.json 从dev_gold.json中随机挑选了500条数据补充后的训练集。
- dev_gold.json 从dev_gold.json中随机挑选了部分数据扩充训练集后剩余的验证集。
- test.json 测试集，无标注。
- data_combine.json train.json + dev_gold.json，用于测试阶段作为基于检索的模型的数据库。
- fin_kb.json 包含了数据库中的各种实体、实体的属性名，以及实体属性的枚举值。也可自行从数据库中进行过滤。
- db_info.json 数据库信息，包含各个数据库的表、列和链接键等信息。


### 数据处理

本项目的核心在于数据处理部分

- data/bm25.py BM25 model
- data/data_utils.py 数据处理的一些工具函数
- data/du_sql.py 解析DuSQL标注结构体并构造输入输出
- data/entity_label_utils.py 利用知识图谱文件，对输入中的实体属性枚举值和实体属性名进行标注，增加额外的type embedding用于训练
- data/multi_task_grounding.py structure grounded training
- data/retrieval.py 构造训练检索模型需要的数据
- data/seq2seq.py 构造纯生成时所需要的数据


## 快速开始

### 安装环境依赖

```
pip install -r requirements.txt
```

### 训练Table Retriever (DPR)

```
srun -N 1 -n 1 -c 1 --gpus-per-task 1 --mem 40G python trainer_slurm_fsdp.py -cp conf/dpr -cn base_v2_0
```

可直接使用我们训练好的DPR直接进行推理

### 使用DPR对训练数据和测试集进行推理

在上一步中，如果没有提前结束，程序会自动在测试集上进行推理，需要首先把`输出目录/test/`目录下的`eval_predictions.json`重命名为可区别的名称，
如`eval_predictions_test.json`。
输出目录可以通过在配置文件中修改`output_dir`来设置。
否则可以按照如下步骤重新执行推理过程：

1. 修改`conf/dpr/base_v2_0.yaml`配置文件中的`do_train`为`False`。
2. 修改`conf/dpr/base_v2_0.yaml`中的`test_file`为需要执行推理的文件，如`sub_train.json`
3. 重新运行训练DPR的命令即可。
4. 推理结束后对`test`目录下的`eval_predictions.json`进行重命名。

### 训练模型

执行以下命令

```
srun -N 1 -n 1 -c 4 --gpus-per-task 4 --mem 40G python trainer_slurm_fsdp.py -cp conf/mt5/xl -cn cls_table_dpr_en_col_en_gd_v2_1_full2_type
```

配置文件中需要检查`do_train=True`以及训练、验证、测试文件路径是否正确；对应的DPR的推理结果的路径是否正确；数据库信息文件路径是否正确；
训练完成后可以在此基础上使用retrieval模式进行微调，即添加训练集中的训练样本作为示例参与训练和预测，执行命令：
```
srun -N 1 -n 1 -c 4 --gpus-per-task 4 --mem 40G python trainer_slurm_fsdp.py -cp conf/mt5/xl -cn cls_table_dpr_en_col_en_gd_exp_v4_0_full2_type
```
需要将配置文件中的`model_name_or_path`域修改为上一步中在非检索模式下训练好的模型的目录。

### 结果提交
输出目录下的`test/eval_predictions.json`可直接提交作为最终结果。

### 模型集成

我们提供了用于模型集成的脚本`scripts/ensemble.py`，可用于将多个模型的预测结果基于beam search的分数进行集成，需要模型在预测是开启beam search模式，设置`num_beams`为大于1的任意数字即可。

需要注意，模型集成依赖预测结果中每个序列的得分，改得分只有在beam search的情况下才会有，因此如果需要模型继承，请在推理阶段开启beam search。训练阶段可以关闭以节省时间。

我们同样提供了其他的消融模型（超参数不同，或在输入上有些许不同），可以用于模型集成，详情请参考代码中README的说明。

## 方法
本部分主要介绍我们具体的方法。

### 模型

模型架构使用Transformer encoder-decoder，预训练权重从mT5-xl初始化，使用mT5的原因是输入的自然语言查询是中文，生成的SQL语句需要用到英文词表。

### 输入输出
End2end生成。输入序列可定义为如下形式：
```
question <table> tab_1 <column> col_1_1 col_1_2 ... <table> tab_2 <column> col_2_1 col_2_2 ... 
```
其中`<table>`和`<column>`都是自定义的特殊符号，会添加到此表中。`tab_i`表示第i个表格的表头，`col_i_j`表示第i个表中的第j列的列名称（英文）。

为了提升最终生成的效果，可以选择添加列名和表名的中文输入。具体请参考相应方法的参数。

另外由于比赛要求模型同时判断当前查询所使用的数据库，在encoder之后添加了一个额外的全连接层进行分类，用于判断使用的数据库（三分类，只有三个数据库）。
在输入序列中添加额外的表名和列名的原因是希望模型更加专注于学习查询内容与表名、列名之间的关联，以及SQL的语法，而无需记忆所有的数据表的列内容。
输出是一个完整的SQL语句。

### 候选数据表检索
比赛官方提供了一个脚本文件，用于将生成的SQL语句解析成SQL关键字对应的结构体：
```
[
  {
    "q_id": 0,
    "question": "统计不同类型基金产品的日回报率最大值",
    "db_name": "ccks_fund",
    "sql_query": "SELECT b.FundType, MAX(a.NVDailyGrowthRate) FROM mf_netvalueperformancehis AS a JOIN mf_fundarchives AS b ON a.InnerCode = b.InnerCode GROUP BY b.FundType;",
    "from": {
      "table_units": [
        "mf_netvalueperformancehis",   // 表1
        "mf_fundarchives"  // 表2
      ],
      "conds": [
        [
          "mf_netvalueperformancehis",
          "InnerCode"   // 关联字段名
        ],
        [
          "mf_fundarchives",
          "InnerCode"  // 关联字段名
        ]
      ]
    },
    "select": [
      false,  // 是否distinct
      [
        null,   // agg操作符
        "mf_fundarchives",  // 表名
        "FundType",   // 字段名
        false   // 是否distinct
      ],
      [
        "max",
        "mf_netvalueperformancehis",
        "NVDailyGrowthRate",
        false
      ]
    ],
    "where": [],  // [[[tab_name, col_name, where_op, value, datetime_op], and, []], and, [[]]]
    "groupBy": [
      [
        "mf_fundarchives",  // 表名
        "FundType"    // 字段名
      ]
    ],
    "having": [],   // [[agg_op, is_distinct, tab_name, col_name, where_op, value],..]
    "orderBy": [],   // [[desc/asc/null, agg_op, is_distinct, tab_name, col_name], ...]
    "limit": null   // null或者数值
  }
]
```

通过结构体进行解析（主要指 from 部分），我们可以得到当前的SQL语句所使用的数据表，从而构造检索的训练样本。
对于每一个question，记在 from 关键字中出现的数据表为正例，其他未出现的数据表为负例，训练向量检索模型（双塔模型，同DPR）。此外，检索器可选BM25，作为冷启动的基础方法。
数据处理部分在`data/retrieval.py`中。

最后我们使用pipeline形式，先用训练好的检索器，对每一条样本在数据库的所有表中检索 top-k 的数据表，然后将这些表的表头和其包含的列名添加到当前样本的输入序列中。

### Structure Grounded Pre-training

该方法主要参考论文[Structure-Grounded Pretraining for Text-to-SQL](https://aclanthology.org/2021.naacl-main.105.pdf)。

文章的核心做法是，在我们所使用的这样的输入序列中，对于每一个候选表名和列名，进行二分类判断，是否会出现在最后的SQL语句中（即二次召回），同时需要识别question中出现的value（具体的用于限制SQL语句的值）。此外，论文还引进了表名和value的匹配loss。
对于数据构造，我们同样依赖解析后的结构体，结构体中出现的所有列名和表名标注为1，其他被检索器召回的表名和列名记为0。对于value识别任务，我们对于输入序列在encoder后的输入做token分类，value所横跨的片段包含的token标注为1，其余token标注为0。对于matching loss，在数据构造的同时，我们会标注出每个value对应输入中的那一列，记该列为正样本，其余列为负样本。
这部分的代码实现请参考`data/multi_task_grounding.py`和`data/data_utils.py`。

对于DuSQL数据集我们也实现了类似的解析方法（见`data/du_sql.py`），但因比赛限制使用其他NL2SQL的标注数据集，我们最终没有提交在首先在DuSQL进行预训练的模型。

### Retrieval-augmented Training
在训练集中利用BM25通过query检索与当前样本相似的样本，将其SQL语句作为示例添加到模型输入序列中，以提升效果。在预测时可以将验证集也加入检索的范围中。
经试验验证，直接使用这个方法训练可能无法取得更好的效果，需要使用基础的方法进行预训练，因此本方法会作为进一步微调的方法。
